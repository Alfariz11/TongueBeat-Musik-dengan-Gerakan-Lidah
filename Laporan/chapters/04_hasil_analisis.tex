% BAB IV - PENJELASAN CODE
\section{Penjelasan Code}

Bagian ini menjelaskan breakdown dari code-code penting dalam aplikasi Gestune beserta fungsinya.

\subsection{Arsitektur Aplikasi}

Aplikasi Gestune terdiri dari beberapa modul utama:

\begin{itemize}
    \item \textbf{main.py}: Entry point aplikasi dan manajemen lifecycle
    \item \textbf{hand\_tracker.py}: Deteksi dan tracking gerakan tangan menggunakan MediaPipe
    \item \textbf{audio\_engine.py}: Engine audio terpadu untuk sintesis suara dan drum sequencing
    \item \textbf{gesture\_processor.py}: Bridge antara hand tracking dan music generation
    \item \textbf{gestune\_ui.py}: User interface menggunakan PyQt6
\end{itemize}

\subsection{Main Application (main.py)}

File \texttt{main.py} berfungsi sebagai entry point aplikasi dan mengelola lifecycle keseluruhan sistem.

\subsubsection{Inisialisasi Aplikasi}

\begin{lstlisting}[language=Python, caption=Inisialisasi komponen utama]
class GestuneApplication:
    def initialize(self) -> bool:
        # Create Qt application
        self.app = QApplication(sys.argv)
        self.app.setApplicationName("Gestune")
        
        # Create main window
        self.window = GestuneUI()
        
        # Initialize gesture processor
        self.processor = GestureProcessor()
        
        # Connect signals
        self._connect_signals()
        
        return True
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item Membuat Qt application dengan \texttt{QApplication} untuk GUI
    \item Inisialisasi \texttt{GestuneUI} sebagai window utama
    \item Membuat \texttt{GestureProcessor} yang akan running di background thread
    \item Menghubungkan signals antara processor dan UI
\end{itemize}

\subsubsection{Signal Connections}

\begin{lstlisting}[language=Python, caption=Koneksi signals untuk komunikasi antar komponen]
def _connect_signals(self):
    # Processor -> UI signals
    self.processor.frame_processed.connect(
        self.window.update_camera_feed
    )
    
    self.processor.drum_hit.connect(
        self._on_drum_hit
    )
    
    # UI -> Processor signals
    self.window.bpm_changed.connect(
        self.processor.set_bpm
    )
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item Menggunakan PyQt6 signals untuk komunikasi thread-safe
    \item Processor mengirim frame yang sudah diproses ke UI
    \item UI mengirim perubahan BPM ke processor
    \item Pattern observer untuk decoupling komponen
\end{itemize}



\subsection{Hand Tracking (hand\_tracker.py)}

Modul ini bertanggung jawab untuk mendeteksi dan melacak gerakan tangan menggunakan MediaPipe.

\subsubsection{Inisialisasi MediaPipe}

\begin{lstlisting}[language=Python, caption=Setup MediaPipe Hands]
class HandTracker:
    def __init__(self, enable_roi: bool = True):
        self.mp_hands = mp.solutions.hands
        
        self.hands = self.mp_hands.Hands(
            model_complexity=1,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.7,
            max_num_hands=2
        )
        
        # Define ROI zones
        self.roi_zones = {
            'Left': ROIZone(
                x_min=0.0, x_max=0.5,
                color=(100, 200, 255),  # Blue
                name='ARPEGGIATOR ZONE'
            ),
            'Right': ROIZone(
                x_min=0.5, x_max=1.0,
                color=(255, 100, 150),  # Pink
                name='DRUM ZONE'
            )
        }
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item \texttt{model\_complexity=1}: Menggunakan full model untuk akurasi maksimal
    \item \texttt{min\_detection\_confidence=0.7}: Hanya hand dengan confidence \u003e 70\% yang dideteksi
    \item \texttt{max\_num\_hands=2}: Tracking maksimal 2 tangan (kiri dan kanan)
    \item ROI zones membagi layar menjadi dua: kiri untuk arpeggiator, kanan untuk drums
\end{itemize}

\subsubsection{Frame Processing}

\begin{lstlisting}[language=Python, caption=Memproses setiap frame untuk deteksi tangan]
def process_frame(self, frame: np.ndarray) -> Dict:
    # Convert BGR to RGB
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # Process with MediaPipe
    self.results = self.hands.process(rgb_frame)
    
    # Process detected hands
    if self.results.multi_hand_landmarks:
        for hand_landmarks, handedness in zip(
            self.results.multi_hand_landmarks,
            self.results.multi_handedness
        ):
            hand_label = handedness.classification[0].label
            
            # Check if hand is in ROI
            in_roi = self._is_hand_in_roi(
                hand_landmarks, hand_label
            )
            
            # Store hand data
            self.hand_data[hand_label] = {
                'landmarks': hand_landmarks,
                'in_roi': in_roi,
                'center_x': center_x,
                'center_y': center_y
            }
    
    return self.hand_data
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item Convert frame dari BGR (OpenCV) ke RGB (MediaPipe)
    \item MediaPipe mendeteksi landmarks (21 titik per tangan)
    \item Classify hand menjadi Left atau Right
    \item Check apakah tangan berada di ROI zone yang sesuai
    \item Return dictionary berisi data semua tangan yang terdeteksi
\end{itemize}



\subsubsection{Gesture Recognition}

\begin{lstlisting}[language=Python, caption=Deteksi jari yang terangkat]
def get_fingers_extended(self, hand_label: str) -> List[bool]:
    landmarks = self.hand_data[hand_label]['landmarks'].landmark
    
    # Thumb (horizontal comparison)
    if hand_label == 'Right':
        thumb_extended = landmarks[4].x < landmarks[3].x
    else:
        thumb_extended = landmarks[4].x > landmarks[3].x
    
    # Other fingers (vertical comparison)
    index_extended = landmarks[8].y < landmarks[6].y
    middle_extended = landmarks[12].y < landmarks[10].y
    ring_extended = landmarks[16].y < landmarks[14].y
    pinky_extended = landmarks[20].y < landmarks[18].y
    
    return [thumb_extended, index_extended, 
            middle_extended, ring_extended, pinky_extended]
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item Untuk \textbf{ibu jari}: membandingkan posisi x (horizontal) karena ibu jari membuka ke samping
    \item Untuk \textbf{jari lainnya}: membandingkan posisi y (vertical) - tip harus di atas PIP joint
    \item Return list of booleans: [thumb, index, middle, ring, pinky]
    \item Digunakan untuk kontrol drum machine (setiap jari = drum berbeda)
\end{itemize}

\subsubsection{Pinch Gesture Detection}

\begin{lstlisting}[language=Python, caption=Mendeteksi jarak pinch untuk volume control]
def get_pinch_distance(self, hand_label: str) -> float:
    thumb = self.hand_data[hand_label]['thumb_tip']
    index = self.hand_data[hand_label]['index_tip']
    
    # Calculate 3D Euclidean distance
    distance = np.sqrt(
        (thumb.x - index.x)**2 +
        (thumb.y - index.y)**2 +
        (thumb.z - index.z)**2
    )
    
    # Apply smoothing
    return self._smooth_value(f'{hand_label}_pinch', distance)
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item Menghitung jarak 3D antara ujung ibu jari dan telunjuk
    \item Menggunakan formula Euclidean distance dengan komponen x, y, dan z
    \item Smoothing diterapkan untuk mengurangi jitter/noise
    \item Jarak ini dimap ke volume (pinch rapat = volume kecil)
\end{itemize}



\subsection{Audio Engine (audio\_engine.py)}

Modul ini menggantikan \texttt{arpeggiator.py} dan \texttt{drum\_machine.py} menjadi satu engine audio terpadu yang menangani sintesis suara dan drum sequencing secara real-time.

\subsubsection{Initialization \& Scheduler}

\begin{lstlisting}[language=Python, caption=Inisialisasi Audio Engine dan Scheduler]
class AudioEngine:
    def __init__(self, assets_path=None):
        # Initialize mixer
        pygame.mixer.init(frequency=44100, size=-16, channels=2, buffer=512)
        
        # Load Drums
        self.drums = {}
        # ... load wav files ...

        # Synth State
        self.scale = self._generate_scale_frequencies()
        self.synth_sounds = self._precompute_synth_sounds()
        
        # Scheduler State
        self.bpm = 100
        self.step_duration = (60 / self.bpm) / 4 # 16th notes
        
        # Start Scheduler Thread
        self.scheduler_thread = threading.Thread(
            target=self._scheduler_loop, 
            daemon=True
        )
        self.scheduler_thread.start()
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item Menggunakan satu thread terpisah (\texttt{scheduler\_loop}) untuk timing yang presisi
    \item Menggabungkan loading aset drum dan pre-computation suara synth
    \item Mengatur BPM dan durasi step untuk sinkronisasi
\end{itemize}

\subsubsection{Synthesis \& Precomputation}

\begin{lstlisting}[language=Python, caption=Pre-compute suara synthesizer]
def _precompute_synth_sounds(self):
    sounds = []
    for freq in self.scale:
        # Generate Sine Wave
        t = np.linspace(0, duration, int(sample_rate * duration), False)
        wave = np.sin(2 * np.pi * freq * t)
        
        # Simple Envelope (Attack/Decay)
        envelope = np.concatenate([
            np.linspace(0, 1, int(sample_rate * 0.01)), # Attack
            np.linspace(1, 0, int(sample_rate * (duration - 0.01))) # Decay
        ])
        wave = wave * envelope
        
        # Convert to Sound object
        sound = pygame.sndarray.make_sound(stereo)
        sounds.append(sound)
    return sounds
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item Suara disintesis di awal (pre-compute) untuk mengurangi latency saat runtime
    \item Menggunakan gelombang sinus sederhana dengan envelope attack/decay
    \item Disimpan dalam list \texttt{synth\_sounds} yang dipetakan ke index nada
\end{itemize}

\subsubsection{Sequencer Loop}

\begin{lstlisting}[language=Python, caption=Loop utama sequencer]
def _play_step(self, step):
    with self.lock:
        # Play Drums
        for drum_name in self.active_drums:
            if self.drum_pattern[drum_name][step]:
                if drum_name in self.drums:
                    self.drums[drum_name].play()
        
        # Play Arpeggios
        for hand_idx, arp_data in self.active_arpeggios.items():
            root_idx = arp_data['note_index']
            # Simple Arpeggio Pattern
            offsets = [0, 2, 4, 2] 
            offset = offsets[step % 4]
            
            note_idx = root_idx + offset
            if 0 <= note_idx < len(self.synth_sounds):
                sound = self.synth_sounds[note_idx]
                sound.play()
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item \texttt{\_play\_step} dipanggil setiap 16th note
    \item Mengecek \texttt{active\_drums} (drum yang sedang diaktifkan user)
    \item Memainkan arpeggio berdasarkan root note yang dipilih user
\end{itemize}



\subsection{Gesture Processor (gesture\_processor.py)}

Modul yang menghubungkan hand tracking dengan music generation, berjalan di background thread.

\subsubsection{Main Processing Loop}

\begin{lstlisting}[language=Python, caption=Loop utama pemrosesan gesture]
def run(self):
    while self.running:
        ret, frame = self.cap.read()
        if not ret:
            continue
        
        # Mirror frame
        frame = cv2.flip(frame, 1)
        
        # Process frame
        self._process_frame(frame)
        
        # Calculate FPS
        self._update_fps()
        
        # Emit processed frame
        self.frame_processed.emit(frame)
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item Loop berjalan di QThread terpisah (tidak blocking UI)
    \item Frame di-mirror untuk user experience yang lebih natural
    \item \texttt{\_process\_frame} melakukan hand tracking dan music generation
    \item Frame yang sudah diproses dikirim ke UI via signal
\end{itemize}

\subsubsection{Arpeggiator Control}

\begin{lstlisting}[language=Python, caption=Kontrol arpeggiator dengan tangan kiri]
def _process_arpeggiator(self, hand_info: Dict, frame_shape: Tuple):
    # Get hand height (for pitch)
    hand_height = 1.0 - hand_info['wrist_y']
    
    # Get pinch distance (for volume)
    pinch_distance = self.tracker.get_pinch_distance(HandSide.LEFT.value)
    
    # Map height to note index
    if self.audio:
        num_notes = len(self.audio.scale)
        note_index = int(hand_height * num_notes)
        note_index = max(0, min(note_index, num_notes - 1))
        
        # Map pinch to volume
        volume = max(0.0, min(1.0, (1.0 - pinch_distance)))
        
        # Update arpeggiator in AudioEngine
        self.audio.update_arpeggio("left_hand", note_index, volume)
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item \textbf{Hand height} dimap ke index nada dalam skala pentatonik
    \item \textbf{Pinch distance} dimap ke volume
    \item \texttt{AudioEngine} menangani playing dan timing dari arpeggio
\end{itemize}

\subsubsection{Drum Control}

\begin{lstlisting}[language=Python, caption=Kontrol drum machine dengan tangan kanan]
def _process_drums(self, hand_info: Dict, frame_shape: Tuple):
    # Get finger extension status
    fingers = self.tracker.get_fingers_extended(HandSide.RIGHT.value)
    
    # Map fingers to drums
    active_drums = set()
    drum_map = {0: 'kick', 1: 'snare', 2: 'hihat', 3: 'clap', 4: 'clap'}
    
    for i, is_extended in enumerate(fingers):
        if is_extended and i in drum_map:
            active_drums.add(drum_map[i])
    
    # Update active drums in AudioEngine
    if self.audio:
        self.audio.update_drums(active_drums)
\end{lstlisting}

\textbf{Penjelasan}:
\begin{itemize}
    \item Setiap jari dipetakan ke instrumen drum tertentu
    \item Jika jari terangkat, instrumen tersebut dimasukkan ke \texttt{active\_drums}
    \item \texttt{AudioEngine} akan memainkan instrumen yang aktif sesuai pattern sequencer
\end{itemize}

\newpage